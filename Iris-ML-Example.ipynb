{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa66af3",
   "metadata": {},
   "source": [
    "# Python ML Lab\n",
    "\n",
    "Purpose of this document is to step you through building and running a \"simple\" machine learning algorithm. Each block of code is commented with explinations for the steps.\n",
    "\n",
    "We are going to use 4 different features to predict that variety (species) of irises. The values of the features are going to be numeric.\n",
    "\n",
    "If you do have any issues installing libraries, please feel free to reach out by email: l.m.sharp215@gmail.com. \n",
    "\n",
    "So without further ado, lets get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3c0464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard python libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# numpy for math\n",
    "# pandas for managing data sets with multiple features and some plotting routines (pairwise plots for example)\n",
    "# matplotlib for plotting\n",
    "# Seaborn has advanced plotting routines (violin plot and pairwise plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfc570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit learn - this is the machine learning library that will be used\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# metrics for evaluations\n",
    "# KNeighbors ML learning method\n",
    "# LogisticRegression used for testing -> testing how often KNN is true or false\n",
    "# train_test_split is to randomly split your data to minimize learning errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ffddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The iris data set, csv -> comma seperated variable\n",
    "# The next commands all use the pandas library\n",
    "# It has multiple features, sepal length, width, petal length, width and variety (species)\n",
    "\n",
    "data = pd.read_csv('iris.csv')\n",
    "# Head prints out the first 5 rows of data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c145c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provies information on the data,the features (columns), the rows/feature (data), and data tyoe for each column\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a839c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe gives statistical information for each feature\n",
    "# What are the mean, std, min.. for each feature\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b302cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is our iris data, c=data[...] tells the function what feature to color\n",
    "# Pairwise plot plots a feature compared to another. \n",
    "# Diagons are histograms of a single feature and off diagnals are cross features\n",
    "#\n",
    "# The figures plotted shows how data clusters or the distribution of traits\n",
    "# Look at the first row, second collum -- it ask the question is there \n",
    "# a relationship beteen sepal length and sepal width\n",
    "\n",
    "grr = pd.plotting.scatter_matrix(data, c=data[\"petal.width\"], figsize=(15, 15), marker='o',\n",
    "                        hist_kwds={'bins': 20}, s=60, alpha=.8)\n",
    "plt.show()\n",
    "\n",
    "# Do you see any correlations? What are the average values of the histograms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f38918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The violin plot will compare histograms of two features\n",
    "# Useful to find means, medians, modes, and unexpected data\n",
    "# Personally I find reading these a little easier to read when comparing features\n",
    "\n",
    "# The first figure shows the varriety in sepal.length for a given variety of iris\n",
    "\n",
    "g = sns.violinplot(y='variety', x='sepal.length', data=data, inner='quartile')\n",
    "plt.show()\n",
    "g = sns.violinplot(y='variety', x='sepal.width', data=data, inner='quartile')\n",
    "plt.show()\n",
    "g = sns.violinplot(y='variety', x='petal.length', data=data, inner='quartile')\n",
    "plt.show()\n",
    "g = sns.violinplot(y='variety', x='petal.width', data=data, inner='quartile')\n",
    "plt.show()\n",
    "\n",
    "# How do the average length/widths change for each variety of Iris?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d470d9aa",
   "metadata": {},
   "source": [
    "OK, we've looked through some of the data so far. The data we've looked at provides statistical information and compares two or more distributions.\n",
    "\n",
    "We can make some predictions based on it, however it may take us time to dig through all of it. Instead, we wil implement machine learning (specifically supevised learning using K nearest neighbors). \n",
    "\n",
    "What we want to do know is isolate one feature, and use it to train a machine learning model to predict species of an Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X contains all features\n",
    "# y is the label \n",
    "\n",
    "X = data.drop([\"variety\"],axis=1)\n",
    "y = data[\"variety\"]\n",
    "\n",
    "# consider what the shapes are\n",
    "# are X and y a matrix or a vector?\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting with different k values\n",
    "# k is a hyperparameter, tuning for a specific k is important\n",
    "# And yes we are tuning as we train, in this example is saves time\n",
    "\n",
    "k_range = list(range(1,50))\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    # model we are using is Kneighbors - used for classificaiton\n",
    "    # n_neighbors -> setting the number of neighbors to compare\n",
    "    \n",
    "    # select the model, input hyperparameter\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # train the model,  \n",
    "    knn.fit(X, y)\n",
    "    \n",
    "    # check how well your model predicts your data\n",
    "    y_pred = knn.predict(X)\n",
    "    \n",
    "    # quantify your results, determines the number of accuratly predicted values\n",
    "    # comparing the predicted values to the training values (how often is it right?)\n",
    "    scores.append(metrics.accuracy_score(y, y_pred))\n",
    "    \n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('Value of k for KNN')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')\n",
    "plt.show()\n",
    "# How does your accuracy varry with number of neighbors? What are the best values of k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The previous data trainied the model with ALL the data\n",
    "\n",
    "# Using train_test_split will randomly split up your data into\n",
    "# training and testing sets\n",
    "# Remember, splitting up your data is also a hyperparameter!\n",
    "\n",
    "# test_size = % data used (yes bit high)\n",
    "# random_state = how it splits up data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=5)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa366cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code is almost the same as above\n",
    "# The difference is we don't use the FULL data set\n",
    "# only the training subset\n",
    "\n",
    "# experimenting with different n values\n",
    "k_range = list(range(1,50))\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    scores.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    \n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('Value of k for KNN')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')\n",
    "plt.show()\n",
    "\n",
    "# Is the data better or worse? What is the \"best\" values of k?\n",
    "# What happens if you use different sizes of X_train (ie, more or less than what you are currently using?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed3b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logreg is in itself a model. We are using it\n",
    "# to evaluate our testing data compared to your ground truth\n",
    "# We are asking how oftern does our model predict correctly \n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# metrics.accuracy_score(y_test, y_pred) / 1? closer to 1 the better it is\n",
    "# Try re-running the logreg test for the model that uses ALL the data, how well does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88593a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at that! You've worked to optimize a model to predict \n",
    "# iris species!\n",
    "\n",
    "# Run the prediction block (this) and see how well it guesses your iris species\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=12)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# make a prediction for an example of an out-of-sample observation\n",
    "pred = knn.predict([[5, 3, 1, .5]])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f01ded1",
   "metadata": {},
   "source": [
    "For the very last test, take a look at the data, choose a species (not the one above), and fill in the knn.predict([v1,v2,v3,v4]) with numbers representing a difference species.\n",
    "\n",
    "If our model is ''ok'', it should accuratly guess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28408967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets confirm that it can choose another variety of Iris\n",
    "# Using the average values for sepal and petal lengths and widths or Virginica\n",
    "# can our model predict the right species?\n",
    "# ie ~ [7,3,5,2]\n",
    "\n",
    "pred = knn.predict([[7,3,5,2]])\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079afca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
